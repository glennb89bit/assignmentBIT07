{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e379305-53f9-49b2-849a-14dbe34f6f64",
   "metadata": {},
   "source": [
    "# Assignment Machine Learning\n",
    "***Assignment BIT07 / Bioinformatics @ home***\n",
    "For this assignment, a dataset on breast cancer characteristics, RNA expression and survival was chosen, because of the medical background \n",
    "\n",
    "This dataset has a mixture of categorical variables (both ordinal and nominal) and numerical variables. It has the possibility to predict survival in both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b48f4-d725-4df9-90df-ae0dcd7c7706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeren van alle modules, functies en methodes\n",
    "###################################################\n",
    "# Data analyse en wetenschappelijke berekeningen\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualisatie (1 of beide; seaborn is mooi alternatief)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = 20,8.27 #grootte van de figuren in inch\n",
    "\n",
    "# Machine learning modules\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, det_curve, balanced_accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Eigen functies\n",
    "################\n",
    "def listdiff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif\n",
    "\n",
    "\n",
    "def ohe_and_bind(original_dataframe, features_to_encode):\n",
    "    if type(features_to_encode) is not list:\n",
    "        raise Exception(\"Features to encode needs to be a list\")\n",
    "\n",
    "    for f_encode in features_to_encode:\n",
    "        dummies = pd.get_dummies(original_dataframe[[f_encode]])\n",
    "        original_dataframe = original_dataframe.drop(f_encode, axis=1)\n",
    "        original_dataframe = pd.concat([original_dataframe, dummies], axis=1)\n",
    "\n",
    "    return(original_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800b8bb6-a9a1-4592-a86f-2a2d6535f906",
   "metadata": {},
   "source": [
    "## Stap 1: Inlezen van de dataset en vooranalyse\n",
    "Volgende stappen moeten worden doornomen:\n",
    "1. Inlezen van de dataset\n",
    "2. Beschrijving van de dataset\n",
    "3. Zoeken naar missing values\n",
    "4. Zoeken naar inconsistente waarden\n",
    "5. Zoeken naar uitschieters\n",
    "6. Verwijderen of omvormen van data\n",
    "\n",
    "Op het einde een beslissing maken of je deze waarden verwijderdt, behoudt of omvormt.\n",
    "\n",
    "Je kan in 2 richtingen verwijderen: features en observaties. Als de vreemde waarden vooral binnen 1 feature voorvalt, dan is het beter deze kolom te verwijderen. Als de vreemde waarden eerder per observatie/patiënt wordt gezien is het beter om rijen te verwijderen. Bij twijfel eerder rijen verwijderen. Zorg dat er op het einde genoeg rijen overblijven. \n",
    "\n",
    "Na alle data gecontroleerd te hebben, moet je features en targets vaststellen (voor zowel regressie als classificiatie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ddf68a-42f5-4cc8-a93c-f42c7943468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.1: Inlezen van de dataset\n",
    "df = pd.read_csv('file.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b655a7c4-24b2-47d8-91d2-03da47225560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.2: beschrijven van de dataset\n",
    "df.shape #dimensies van de dataset\n",
    "df.describe() # range van waarden; vul ook aan met info over kolommen van bron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f42a9f-1782-41e2-81f5-5251390b51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.3: Zoeken naar missing values\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9fb1f8-61bb-4318-8cca-d45df7a3bed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.4: Zoeken naar inconsistente\n",
    "df.describe() # range van waarden kloppen met mogelijke werkelijkheid? Te veel nullen als missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d912e9-2cf6-489e-81df-a0f55989ec3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.5: Zoeken naar uitschieters\n",
    "columns = df.columns()\n",
    "\n",
    "for column in columns:\n",
    "    ax1 = sns.boxplot(x=column, data=df) #interessanter voor categorische data\n",
    "    ax2 = sns.histplot(x=column, data=df) #beter voor numerieke data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e0b7b-0976-4c5f-a9ee-bd362f3ae8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 1.6: Verwijderen van data\n",
    "df.drop('column', axis=1, inplace=True) #kolom verwijderen wegens niet contributief of missing values\n",
    "df.drop(df[(df.column1 == 0) | (df.column2 == 0) | (df.column3 == 0)].index, inplace = True) #Rijen verwijderen met foutieve waarde 0\n",
    "df = df[(np.abs(stats.zscore(df)) < 5).all(axis=1)] #Verwijderen van uitschieters die meer dan 5x SD verwijderd zijn van gemiddelde\n",
    "df = df.dropna() #Verwijderen van rijen met missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c900bad-8fb4-4c2b-bd64-1498e14b3428",
   "metadata": {},
   "source": [
    "### Conclusie stap 1\n",
    "Deze dataset bestaat uit volgende kolommen (+info):\n",
    "- Kolom 1: info\n",
    "- Kolom 2: info\n",
    "- ...\n",
    "\n",
    "Er zijn X observaties en Y kolommen. Hierin zijn er A vreemde waarden/missing values. Deze worden verwijderd/behouden/omvormd. Omvormen (imputing) kan pas na het splitsen van de dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d81e-e5e3-4a05-8c92-7150e56f36d7",
   "metadata": {},
   "source": [
    "## Stap 2: Analyse en preprocessing\n",
    "**Stap 2.1: analyse**  \n",
    "Bij de analytische fase is het de bedoeling om te kijken naar welke features een invloed hebben op het target. Dit kan op volgende manieren:\n",
    "- Pairplot (moeilijk leesbaar bij veel features --> exporteren en zoomen)\n",
    "- Heatmap (moeilijk leesbaar bij veel features --> exporteren en zoomen)\n",
    "- Afzonderlijke scatterplots\n",
    "- Boxplots van categorieke features\n",
    "- Gebalanceerdheid van de data\n",
    "\n",
    "Creativiteit de vrije loop laten gaan!\n",
    "\n",
    "**Stap 2.2 preprocessing**  \n",
    "Voor het preprocessen zijn er verschillende stappen te ondergaan:\n",
    "1. Omvormen van ordinale categorieën (ordinal encoding)\n",
    "2. Omvormen van categorische waarden (one-hot encoding)\n",
    "3. Opsplitsen in features en targets\n",
    "4. Opsplitsen in testset en trainingsset\n",
    "4. Omvormen van missing values (imputing)\n",
    "5. Schalen van numerieke features\n",
    "\n",
    "Bij elke stap laten weten op welke manier en waarom je dit doet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb651ad7-fe65-4739-af91-8336cdd3698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.1.1: Pairplot\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45930b81-6902-471d-88fa-4599f5378028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.1.2 Heatmap\n",
    "df_corr = df.corr()\n",
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(df_corr, \n",
    "            mask=np.zeros_like(df_corr, dtype=bool), \n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True),\n",
    "            square=True, \n",
    "            ax=ax,annot=True\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8dc21-287b-4622-bd30-d3b38e0e7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.1.3: Afzonderlijke scatterplots\n",
    "sns.scatterplot(data=df, x='feature', y='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b06dcf-c6c3-4960-834e-15d564067a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.1.4: Boxplots van categorieke waarden\n",
    "sns.boxplot(x=df[\"feature\"], y=df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878008bd-1c9a-4028-be48-983985547229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.1.5: Gebalanceerdheid van de data op basis van target\n",
    "df['target'].value_counts() #absolute waarden\n",
    "df['target'].value_counts(normalize=True) #percentages\n",
    "sns.countplot(x=\"target\", data=df) #histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55200212-a2a5-4208-94b7-566340c7f4f2",
   "metadata": {},
   "source": [
    "### Conclusie stap 2.1\n",
    "Formuleer enkele bevindingen van de data-analyse. Kopieer deze ook al naar de eindconclusie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208b39e-d8d1-4f0c-b04b-6d21f6817938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.1: Omvormen ordinale categorieën: rankings naar nummers\n",
    "cat1 = ['rank1', 'rank2', 'rank3'] # categoriesleutel op basis van de vooranalyse voor feature 1\n",
    "cat2 = ['rank1', 'rank2', 'rank3'] # categoriesleutel op basis van de vooranalyse voor feature 2\n",
    "\n",
    "ord_features = ['feature1', 'feature2'] # lijst met alle colomnamen met ordinale features\n",
    "rest_features = listdiff(df.columns, ord_features)\n",
    "\n",
    "ordinal_trf = ColumnTransformer(\n",
    "    transformers =[\n",
    "        ('ord', #gewoon een naam\n",
    "         preprocessing.OrdinalEncoder(categories=[cat1, cat2]), # Belangrijk!: er moeten evenveel categorieën zijn als aantal om te zetten features. Deze moeten in volgorde staan en als twee features zelfde categorieën hebben mag dit herhaald worden\n",
    "         ord_features), #Lijst met de features om te transformeren\n",
    "], remainder ='passthrough')\n",
    "\n",
    "df = pd.DataFrame(ordinal_trf.fit_transform(df), columns=[ord_features, rest_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78215b6-4e2a-4b51-8c71-ccb9660826b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.2: One-hot encoding\n",
    "ohe_features = [featureA, featureB]\n",
    "df = ohe_and_bind(df, ohe_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f80b8f-dd68-4447-b9fa-77da0ac208f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.3: Opsplitsen in features en target\n",
    "y = df['target'].values.astype('int')\n",
    "X = pd.DataFrame(df.drop('target', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e59a7f-c0ab-4826-bff1-78716f7a0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.4: Opsplitsen in training en testset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=200, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c84221-f96f-49e0-b201-6ef653f9d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.5: Imputing (kan wegvallen indien niet nodig)\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean') # of andere strategie\n",
    "imp.fit(X_train)\n",
    "\n",
    "imp.transform(X_train)\n",
    "X_train = pd.DataFram(X_train, columns=imp.feature_names_in_)\n",
    "imp.transform(X_test)\n",
    "X_test = pd.DataFram(X_test, columns=imp.feature_names_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4da5e-c63d-489a-a592-4bebf4a346b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap 2.2.6: Schaling (moet gebeuren!)\n",
    "scaler = StandardScaler() # of andere schaler\n",
    "scaler.fit(X_train)\n",
    "scaler.transform(X_train)\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a932a953-608d-4186-b552-3add306ff28b",
   "metadata": {},
   "source": [
    "### Conclusie stap 2.2\n",
    "Maak een conclusie over alle preprocessing eventueel met vermelding van het aantal features nu. Ook altijd vermelden waarom je voor welke techniek hebt gekozen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf4e416-6b80-4ae9-84e8-a85401a6b281",
   "metadata": {},
   "source": [
    "## Stap 3: Regressie\n",
    "Gebruik lineaire regressie om een conintue variabele target te voorspellen.\n",
    "\n",
    "Stappenplan:\n",
    "1. Initieel model zonder tweaking\n",
    "2. Initieel model beoordelen (MSE, MAE en R²)\n",
    "3. Model tunen\n",
    "    1. Nieuwe features creëren\n",
    "    2. Polynomiale expansie\n",
    "    3. Regularisatie\n",
    "    4. Hyperparameter tuning: Grid search/Random search\n",
    "4. Model selecteren op basis van Grid search\n",
    "5. Finaal model evalueren\n",
    "\n",
    "Het is moeilijk om hier voorbeeldcode te geven. Dit moet bijna altijd worden aangepast aan de situatie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197be56-bf12-4a9f-b5e4-8c670aafab22",
   "metadata": {},
   "source": [
    "## Stap 4: Classificatie\n",
    "Gebruik Logistische regressie, SVM om een categorieke variabele target te voorspellen.\n",
    "\n",
    "Stappenplan (zelfde als bij lineaire regressie, maar meerdere type modellen te testen bij Grid search):\n",
    "1. Initieel model zonder tweaking\n",
    "2. Initieel model beoordelen (Precisie, Recall, Accuracy, Confusiematrix, ROC en AUC)\n",
    "3. Model tunen\n",
    "    1. Nieuwe features creëren\n",
    "    2. Polynomiale expansie\n",
    "    3. Regularisatie\n",
    "    4. Hyperparameter tuning: Grid search/Random search\n",
    "4. Model selecteren op basis van Grid search\n",
    "5. Finaal model evalueren\n",
    "\n",
    "Het is moeilijk om hier voorbeeldcode te geven. Dit moet bijna altijd worden aangepast aan de situatie. Bij Classificatie is het altijd leuk om de ROC curves te tekenen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301f95f-43f2-477c-b08d-c91d8a46f3fc",
   "metadata": {},
   "source": [
    "## Stap 5: Clustering\n",
    "Gebruik een clustering methode om een cluster te voorspellen.\n",
    "\n",
    "Stappenplan (zelfde als bij lineaire regressie, maar meerdere type modellen te testen bij Grid search):\n",
    "1. Initieel model zonder tweaking\n",
    "2. Initieel model beoordelen (nog op te zoeken: evaluatieparameters)\n",
    "3. Model tunen\n",
    "    1. Nieuwe features creëren\n",
    "    2. Polynomiale expansie\n",
    "    3. Regularisatie\n",
    "    4. Hyperparameter tuning: Grid search/Random search (? geen idee of dit past)\n",
    "4. Model selecteren op basis van Grid search\n",
    "5. Finaal model evalueren\n",
    "\n",
    "Het is moeilijk om hier voorbeeldcode te geven. Dit moet bijna altijd worden aangepast aan de situatie. We moeten blijkbaar niet alle attributen gebruiken in het model. We mogen ons beperken tot de attributen met een goede performance of een combinatie van verschillende goede attributen. We moeten het wel proberen te baseren op dezelfde attributen als bij classificatie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de8f74-0599-4ceb-a936-93fc34b32b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap x.1: Initieel model\n",
    "model = module.Methode(parameters) # Aan te passen voor het type model je wilt gebruiken\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "## Voorbeelden modellen:\n",
    "lregmodel = linear_model.LinearRegression()\n",
    "ridgemodel = linear_model.Ridge(alpha=alpha,tol=0.0001,fit_intercept=True) # L2 regularisatie\n",
    "lasso = linear_model.Lasso(alpha=alpha,tol=0.0001,fit_intercept=True) # L1 regularisatie\n",
    "logrmodel = linear_model.LogisticRegression(C=1, solver='liblinear', class_weight='balanced', multi_class='ovr')\n",
    "svmmodel = svm.SVC(kernel='linear',C=0.01)\n",
    "# Wordt nog verder aangevuld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ff9d7-e384-40e3-a423-057c23c2a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap x.2: Model beoordelen\n",
    "## Linear, Ridge en Lasso\n",
    "r2train = lregmodel.score(X_train, y_train)\n",
    "r2test = lregmodel.score(X_test, y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "## Logistic en SVM\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "## Clustering\n",
    "# Wordt nog aangevuld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53de9e-9805-4d1a-8d09-17cd94473fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stap x.3: Model tunen (werk opnieuw vanaf de basisset)\n",
    "## x.3.1: Nieuwe features creëren\n",
    "df.insert(df.columns.size-1, 'new_feature', df.feature1*df.feature2) # kan ook som, min, deling\n",
    "\n",
    "\n",
    "## x.3.2: Polynomiale expansie\n",
    "pol = df[['lijst met features met continue variabelen']]\n",
    "nopol = lego.drop(['lijst met features met continue variabelen'], axis=1)\n",
    "\n",
    "graad = 4 \n",
    "\n",
    "poly = PolynomialFeatures(graad)\n",
    "poly.fit(pol)\n",
    "polynom = pd.DataFrame(poly.transform(pol), columns=poly.get_feature_names_out(pol.columns))\n",
    "\n",
    "### Belangrijk! vul aan met One-hot encoding en ordinal encoding indien nodig, daarna opnieuw opsplitsen\n",
    "ord_features = ['feature1', 'feature2'] # lijst met alle colomnamen met ordinale features\n",
    "rest_features = listdiff(df.columns, ord_features)\n",
    "nopol = pd.DataFrame(ordinal_trf.fit_transform(nopol), columns=[ord_features, rest_features])\n",
    "\n",
    "ohe_features = [featureA, featureB]\n",
    "nopol = ohe_and_bind(nopol, ohe_features)\n",
    "\n",
    "df_preprocessed = pd.concat([polynom, nopol], axis=1)\n",
    "\n",
    "y = df_preprocessed['target'].values.astype('int')\n",
    "X = pd.DataFrame(df_preprocessed.drop('target', axis=1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=200, random_state=0)\n",
    "\n",
    "\n",
    "## X.3.3: Regularisatie (kan ook tijdens gridsearch)\n",
    "### Linear regression --> gebruik Lasso of Ridge regressie\n",
    "### Logistic regression --> standaard is L2\n",
    "lr = linear_model.LogisticRegression(C=1, solver='liblinear', class_weight='balanced', penalty='l1')\n",
    "### SVM heeft geen regularisatie maar wel kernels\n",
    "svmmodel = svm.SVC(kernel='poly',C=10000000)\n",
    "\n",
    "\n",
    "## X.3.4: Hyperparameter tuning met grid search of random search\n",
    "### linear regression alleen bij ridge/lasso en weinig zin\n",
    "lrparam = {'alpha' = np.logspace(-5, 4, 10)}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = lrparam,\n",
    "                           scoring = 'r2', # roc_auc, f1_weighted, f1_macro, recall, ...\n",
    "                           cv = 15,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 5)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "y_pred = grid_search.predict(X_test)\n",
    "r2train = lregmodel.score(X_train, y_train)\n",
    "r2test = lregmodel.score(X_test, y_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "### logistic regression grid search\n",
    "logrparam = {\n",
    "    'solver' = ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty' = ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C' = np.logspace(-5, 4, 10)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = logrparam,\n",
    "                           scoring = 'accuracy', # roc_auc, f1_weighted, f1_macro, recall, ...\n",
    "                           cv = 15,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 5)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "y_pred = grid_search.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "### logistic regression random search\n",
    "logrparam = {\n",
    "    'solver' = ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "    'penalty' = ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'C' = loguniform(1e-5, 100)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(model, \n",
    "                                   logrparam, \n",
    "                                   n_iter=20, \n",
    "                                   scoring='accuracy', \n",
    "                                   n_jobs=-1, \n",
    "                                   cv=5, \n",
    "                                   random_state=1)\n",
    "\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "y_pred = random_search.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "### SVM grid search\n",
    "svmparamaters = [ \n",
    "        {'kernel': ['linear'], 'C': np.linspace(0.01,20,10)},\n",
    "        {'kernel': ['rbf'], 'C': np.linspace(0.01,20,10), 'gamma': [0.0001, 0.001, 0.01, 0.1, 0.2]},\n",
    "        {'kernel': ['poly'], 'C':np.linspace(0.01,20,10)} ]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, \n",
    "                           param_grid = svmparamaters,\n",
    "                           scoring = 'accuracy', # roc_auc, f1_weighted, f1_macro, recall, ...\n",
    "                           cv = 15,\n",
    "                           n_jobs = -1,\n",
    "                           verbose = 5)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_ \n",
    "best_parameters = grid_search.best_params_  \n",
    "y_pred = grid_search.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "### SVM random search\n",
    "model = SVC(probability=True)\n",
    "parameters = {'kernel': ['linear','rbf','poly'],\n",
    "              'C': uniform(0.01, 20), # haal C uit een random uniform distribution\n",
    "              'gamma': uniform(0.001, 0.2)}\n",
    "\n",
    "\n",
    "n_iter_search = 20\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=parameters,cv=5,n_iter=n_iter_search,n_jobs = -1,verbose=1)\n",
    "\n",
    "random_search = random_search.fit(X_train, y_train)\n",
    "\n",
    "best_accuracy = random_search.best_score_ \n",
    "best_parameters = random_search.best_params_  \n",
    "y_pred = random_search.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)*100\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "#### Nog aan te vullen voor clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a19ec-38a6-4f59-8983-8f680515ad25",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Conclusie schrijven over:\n",
    "- Dataset\n",
    "    - Goed/slecht en waarom?\n",
    "    - Problemen in de dataset\n",
    "- Model\n",
    "    - Beste model voor elke techniek\n",
    "    - Performance\n",
    "    - Overfitting/underfitting\n",
    "    - Kan gebruikt worden in dagdagelijkse praktijk?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
